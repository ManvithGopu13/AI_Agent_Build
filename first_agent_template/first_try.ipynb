{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdf5e641",
   "metadata": {},
   "source": [
    "## Creating the Basic codeAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4540a734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import CodeAgent,DuckDuckGoSearchTool, HfApiModel,load_tool,tool\n",
    "import datetime\n",
    "import requests\n",
    "import pytz\n",
    "import yaml\n",
    "from tools.final_answer import FinalAnswerTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39152ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Gradio_UI import GradioUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5be12cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def my_custom_tool(arg1:str, arg2:int)-> str: #it's import to specify the return type\n",
    "    #Keep this format for the description / args / args description but feel free to modify the tool\n",
    "    \"\"\"A tool that does nothing yet \n",
    "    Args:\n",
    "        arg1: the first argument\n",
    "        arg2: the second argument\n",
    "    \"\"\"\n",
    "    return \"What magic will you build ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fe62081",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_current_time_in_timezone(timezone: str) -> str:\n",
    "    \"\"\"A tool that fetches the current local time in a specified timezone.\n",
    "    Args:\n",
    "        timezone: A string representing a valid timezone (e.g., 'America/New_York').\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create timezone object\n",
    "        tz = pytz.timezone(timezone)\n",
    "        # Get current time in that timezone\n",
    "        local_time = datetime.datetime.now(tz).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        return f\"The current local time in {timezone} is: {local_time}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching time for timezone '{timezone}': {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a90657e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_answer = FinalAnswerTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c08c83ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HfApiModel(\n",
    "max_tokens=2096,\n",
    "temperature=0.5,\n",
    "model_id='Qwen/Qwen2.5-Coder-32B-Instruct',# it is possible that this model may be overloaded\n",
    "custom_role_conversions=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "744b2c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import LiteLLMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3e2cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ollama = LiteLLMModel(\n",
    "    model_id=\"ollama_chat/qwen2.5:7b\",  # Or try other Ollama-supported models\n",
    "    api_base=\"http://127.0.0.1:11434\",  # Default Ollama local server\n",
    "    api_key=\"1122\",\n",
    "    num_ctx=8192,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6138c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_generation_tool = load_tool(\"agents-course/text-to-image\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40a971b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prompts.yaml\", 'r') as stream:\n",
    "    prompt_templates = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3734d542",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = CodeAgent(\n",
    "    model=model_ollama,\n",
    "    tools=[final_answer], ## add your tools here (don't remove final answer)\n",
    "    max_steps=6,\n",
    "    verbosity_level=1,\n",
    "    grammar=None,\n",
    "    planning_interval=None,\n",
    "    name=None,\n",
    "    description=None,\n",
    "    prompt_templates=prompt_templates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ca5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the capital of India</span>                                                                                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama_chat/qwen2.5:7b ─────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the capital of India\u001b[0m                                                                                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama_chat/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Error in generating model output:</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">litellm.APIConnectionError: Ollama_chatException - Server error </span><span style=\"color: #008000; text-decoration-color: #008000\">'500 Internal Server Error'</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> for url </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'http://127.0.0.1:11434/api/chat'</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">For more information check: </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mError in generating model output:\u001b[0m\n",
       "\u001b[1;31mlitellm.APIConnectionError: Ollama_chatException - Server error \u001b[0m\u001b[32m'500 Internal Server Error'\u001b[0m\u001b[1;31m for url \u001b[0m\n",
       "\u001b[32m'http://127.0.0.1:11434/api/chat'\u001b[0m\n",
       "\u001b[1;31mFor more information check: \u001b[0m\u001b[4;94mhttps://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 0.11 seconds]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 0.11 seconds]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\saish\\AI_Agents_Build\\venv\\lib\\site-packages\\gradio\\queueing.py\", line 715, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"c:\\Users\\saish\\AI_Agents_Build\\venv\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Users\\saish\\AI_Agents_Build\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 2137, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Users\\saish\\AI_Agents_Build\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 1675, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "  File \"c:\\Users\\saish\\AI_Agents_Build\\venv\\lib\\site-packages\\gradio\\utils.py\", line 735, in async_iteration\n",
      "    return await anext(iterator)\n",
      "  File \"c:\\Users\\saish\\AI_Agents_Build\\venv\\lib\\site-packages\\gradio\\utils.py\", line 729, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "  File \"c:\\Users\\saish\\AI_Agents_Build\\venv\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\saish\\AI_Agents_Build\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\saish\\AI_Agents_Build\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"c:\\Users\\saish\\AI_Agents_Build\\venv\\lib\\site-packages\\gradio\\utils.py\", line 712, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "  File \"c:\\Users\\saish\\AI_Agents_Build\\venv\\lib\\site-packages\\gradio\\utils.py\", line 873, in gen_wrapper\n",
      "    response = next(iterator)\n",
      "  File \"c:\\Users\\saish\\AI_Agents_Build\\first_agent_template\\Gradio_UI.py\", line 197, in interact_with_agent\n",
      "    for msg in stream_to_gradio(self.agent, task=prompt, reset_agent_memory=False):\n",
      "  File \"c:\\Users\\saish\\AI_Agents_Build\\first_agent_template\\Gradio_UI.py\", line 145, in stream_to_gradio\n",
      "    total_input_tokens += agent.model.last_input_token_count\n",
      "TypeError: unsupported operand type(s) for +=: 'int' and 'NoneType'\n"
     ]
    }
   ],
   "source": [
    "GradioUI(agent).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d57a95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
